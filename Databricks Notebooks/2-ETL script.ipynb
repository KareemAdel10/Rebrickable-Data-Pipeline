{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e28094c0-8d14-4a0b-9683-4fcc2eef20d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Azure storage account details\n",
    "account_name = \"*************\"\n",
    "account_key = \"*********\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3338edc1-160e-4356-aede-71771202ae20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from datetime import datetime\n",
    "import os\n",
    "spark = SparkSession.builder.appName(\"Rebrickable ETL\").getOrCreate()\n",
    "spark.conf.set(f\"fs.azure.account.key.{account_name}.dfs.core.windows.net\", account_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4429dff-e20a-47d0-a624-7a31040b9743",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Helper function"
    }
   },
   "outputs": [],
   "source": [
    "def rename_save(table, df):\n",
    "    \"\"\"\n",
    "    Renames delta files\n",
    "    \"\"\"\n",
    "    target_dir = f\"abfss://conformed@{account_name}.dfs.core.windows.net/Input/Lego/{table}\"\n",
    "    temp_dir = f\"{target_dir}_temp\"\n",
    "\n",
    "    df.coalesce(1).write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"overwrite\").save(temp_dir)\n",
    "\n",
    "    try:\n",
    "        files_in_temp = dbutils.fs.ls(temp_dir)\n",
    "        \n",
    "        delta_file = [file.path for file in files_in_temp if file.path.endswith(\".parquet\")][0]\n",
    "        # print(delta_file)\n",
    "\n",
    "        final_output_path = f\"{target_dir}/{table}.parquet\"\n",
    "        \n",
    "        dbutils.fs.mv(delta_file, final_output_path)\n",
    "        \n",
    "        dbutils.fs.rm(temp_dir, True)\n",
    "        \n",
    "        print(f\"Successfully saved Delta file as {final_output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in file renaming/moving: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65ea9dbb-54cf-49fc-aa7e-686c078b2696",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "today = datetime.today()\n",
    "year = today.strftime('%Y')\n",
    "month = today.strftime('%m')\n",
    "day = today.strftime('%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "208ff404-b019-4302-a278-567a2eac1376",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Fact_Inventory"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved Delta file as abfss://conformed@atomicatraining.dfs.core.windows.net/Input/Lego/Fact_Inventory/Fact_Inventory.parquet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load inventory datasets\n",
    "inventories_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(f\"abfss://raw-data@{account_name}.dfs.core.windows.net/Unzipped Rebrickable/Lego/inventories/Year={year}/Month={month}/Day={day}/inventories.csv\")\n",
    "\n",
    "inventory_sets_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(f\"abfss://raw-data@{account_name}.dfs.core.windows.net/Unzipped Rebrickable/Lego/inventory_sets/Year={year}/Month={month}/Day={day}/inventory_sets.csv\")\n",
    "\n",
    "inventory_parts_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(f\"abfss://raw-data@{account_name}.dfs.core.windows.net/Unzipped Rebrickable/Lego/inventory_parts/Year={year}/Month={month}/Day={day}/inventory_parts.csv\")\n",
    "\n",
    "inventory_minifigs_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(f\"abfss://raw-data@{account_name}.dfs.core.windows.net/Unzipped Rebrickable/Lego/inventory_minifigs/Year={year}/Month={month}/Day={day}/inventory_minifigs.csv\")\n",
    "\n",
    "inventories_df.createOrReplaceTempView(\"inventories\")\n",
    "inventory_sets_df.createOrReplaceTempView(\"inventory_sets\")\n",
    "inventory_parts_df.createOrReplaceTempView(\"inventory_parts\")\n",
    "inventory_minifigs_df.createOrReplaceTempView(\"inventory_minifigs\")\n",
    "\n",
    "fact_inventory_df = spark.sql(\"SELECT i.id as inventory_id, i.version, is.set_num as set_num, ip.part_num as part_num, ip.color_id as color_id, ip.is_spare as is_spare, im.fig_num as fig_num, is.quantity as sets_quantity, im.quantity as minifigs_quantity , ip.quantity as parts_quantity, True AS is_active, current_timestamp() AS start_date, 'NULL' AS end_date FROM inventories i LEFT JOIN inventory_sets is ON i.id = is.inventory_id LEFT JOIN inventory_parts ip ON i.id =ip.inventory_id LEFT JOIN inventory_minifigs im ON i.id = im.inventory_id ORDER BY i.id\" )\n",
    "\n",
    "# # Show df \n",
    "# fact_inventory_df.show()\n",
    "\n",
    "# num_rows = fact_inventory_df.count()\n",
    "# print(\"Number of rows:\", num_rows)\n",
    "\n",
    "# Write to Delta format\n",
    "# Can be commented out, depending on the stakeholder's requirements\n",
    "fact_inventory_df.coalesce(1).write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"overwrite\").save(f\"abfss://conformed@{account_name}.dfs.core.windows.net/Input/Lego/Fact_Inventory/\")\n",
    "\n",
    "rename_save(table ='Fact_Inventory', df = fact_inventory_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "813fea03-628f-42ac-8fa7-b3aef99cc11c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Dim_Set"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved Delta file as abfss://conformed@atomicatraining.dfs.core.windows.net/Input/Lego/Dim_sets/Dim_sets.parquet\n"
     ]
    }
   ],
   "source": [
    "sets_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(f\"abfss://raw-data@{account_name}.dfs.core.windows.net/Unzipped Rebrickable/Lego/sets/Year={year}/Month={month}/Day={day}/sets.csv\")\n",
    "\n",
    "sets_df.createOrReplaceTempView(\"sets\")\n",
    "\n",
    "dim_sets_df = spark.sql(\"SELECT *, True AS is_active, current_timestamp() AS start_date, 'NULL' AS end_date FROM sets\")\n",
    "\n",
    "# dim_sets_df.show()\n",
    "\n",
    "# Write to Delta format\n",
    "dim_sets_df.coalesce(1).write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"overwrite\").save(f\"abfss://conformed@{account_name}.dfs.core.windows.net/Input/Lego/Dim_sets/\")\n",
    "\n",
    "rename_save('Dim_sets', dim_sets_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6eede02e-81e5-499e-bf90-6c8a3c61cc82",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Dim_themes"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved Delta file as abfss://conformed@atomicatraining.dfs.core.windows.net/Input/Lego/Dim_themes/Dim_themes.parquet\n"
     ]
    }
   ],
   "source": [
    "themes_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(f\"abfss://raw-data@{account_name}.dfs.core.windows.net/Unzipped Rebrickable/Lego/themes/Year={year}/Month={month}/Day={day}/themes.csv\")\n",
    "\n",
    "themes_df.createOrReplaceTempView(\"themes\")\n",
    "\n",
    "dim_themes_df = spark.sql(\"SELECT id as theme_id, name, parent_id, True AS is_active, current_timestamp() AS start_date, 'NULL' AS end_date FROM themes\")\n",
    "\n",
    "# dim_themes_df.show()\n",
    "\n",
    "# Write to Delta format\n",
    "dim_themes_df.coalesce(1).write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"overwrite\").save(f\"abfss://conformed@{account_name}.dfs.core.windows.net/Input/Lego/Dim_themes/\")\n",
    "\n",
    "rename_save('Dim_themes', dim_themes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a6200de-f242-4b56-ae48-01e32896dce1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Dim_parts"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved Delta file as abfss://conformed@atomicatraining.dfs.core.windows.net/Input/Lego/Dim_parts/Dim_parts.parquet\n"
     ]
    }
   ],
   "source": [
    "parts_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(f\"abfss://raw-data@{account_name}.dfs.core.windows.net/Unzipped Rebrickable/Lego/parts/Year={year}/Month={month}/Day={day}/parts.csv\")\n",
    "\n",
    "inventory_parts_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(f\"abfss://raw-data@{account_name}.dfs.core.windows.net/Unzipped Rebrickable/Lego/inventory_parts/Year={year}/Month={month}/Day={day}/inventory_parts.csv\")\n",
    "\n",
    "part_categories_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(f\"abfss://raw-data@{account_name}.dfs.core.windows.net/Unzipped Rebrickable/Lego/part_categories/Year={year}/Month={month}/Day={day}/part_categories.csv\")\n",
    "\n",
    "parts_df.createOrReplaceTempView(\"parts\")\n",
    "inventory_parts_df.createOrReplaceTempView(\"inventory_parts\")\n",
    "part_categories_df.createOrReplaceTempView(\"part_categories\")\n",
    "\n",
    "dim_parts_df = spark.sql(\"SELECT p.part_num as part_num, p.name as part_name, pc.id as part_cat_id, pc.name as part_cat_name, True AS is_active, current_timestamp() AS start_date, 'NULL' AS end_date FROM parts p JOIN part_categories pc ON p.part_cat_id = pc.id\" )\n",
    "\n",
    "# dim_parts_df.show()\n",
    "\n",
    "# Write to Delta format\n",
    "dim_parts_df.coalesce(1).write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"overwrite\").save(f\"abfss://conformed@{account_name}.dfs.core.windows.net/Input/Lego/Dim_parts/\")\n",
    "\n",
    "rename_save('Dim_parts', dim_parts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22a248c8-5d8c-4daf-b50d-4c3bbf558866",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Dim_colors"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved Delta file as abfss://conformed@atomicatraining.dfs.core.windows.net/Input/Lego/Dim_colors/Dim_colors.parquet\n"
     ]
    }
   ],
   "source": [
    "colors_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(f\"abfss://raw-data@{account_name}.dfs.core.windows.net/Unzipped Rebrickable/Lego/colors/Year={year}/Month={month}/Day={day}/colors.csv\")\n",
    "\n",
    "colors_df.createOrReplaceTempView(\"colors\")\n",
    "\n",
    "colors_df = spark.sql(\"SELECT id as color_id, name, rgb, is_trans, True AS is_active, current_timestamp() AS start_date, 'NULL' AS end_date FROM colors\")\n",
    "\n",
    "# colors_df.show()\n",
    "\n",
    "# Write to Delta format\n",
    "colors_df.coalesce(1).write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"overwrite\").save(f\"abfss://conformed@{account_name}.dfs.core.windows.net/Input/Lego/Dim_colors/\")\n",
    "\n",
    "rename_save('Dim_colors', colors_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "725acec1-e409-45e0-8507-9365d1adfeab",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Dim_part_relationships"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved Delta file as abfss://conformed@atomicatraining.dfs.core.windows.net/Input/Lego/Dim_part_relationships/Dim_part_relationships.parquet\n"
     ]
    }
   ],
   "source": [
    "part_relationships_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(f\"abfss://raw-data@{account_name}.dfs.core.windows.net/Unzipped Rebrickable/Lego/part_relationships/Year={year}/Month={month}/Day={day}/part_relationships.csv\")\n",
    "\n",
    "part_relationships_df.createOrReplaceTempView(\"part_relationships\")\n",
    "\n",
    "part_relationships_df = spark.sql(\"SELECT *, True AS is_active, current_timestamp() AS start_date, 'NULL' AS end_date FROM part_relationships\")\n",
    "\n",
    "# part_relationships_df.show()\n",
    "\n",
    "# Write to Delta format\n",
    "part_relationships_df.coalesce(1).write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"overwrite\").save(f\"abfss://conformed@{account_name}.dfs.core.windows.net/Input/Lego/Dim_part_relationships/\")\n",
    "\n",
    "rename_save('Dim_part_relationships', part_relationships_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d20072b7-ef4d-4e51-be7c-55bd419a9bd1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Dim_Minifig"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved Delta file as abfss://conformed@atomicatraining.dfs.core.windows.net/Input/Lego/Dim_minifig/Dim_minifig.parquet\n"
     ]
    }
   ],
   "source": [
    "minifigs_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(f\"abfss://raw-data@{account_name}.dfs.core.windows.net/Unzipped Rebrickable/Lego/minifigs/Year={year}/Month={month}/Day={day}/minifigs.csv\")\n",
    "\n",
    "minifigs_df.createOrReplaceTempView(\"minifigs\")\n",
    "\n",
    "minifigs_df = spark.sql(\"SELECT *, True AS is_active, current_timestamp() AS start_date, 'NULL' AS end_date FROM minifigs\")\n",
    "\n",
    "# minifigs_df.show()\n",
    "\n",
    "# Write to Delta format\n",
    "minifigs_df.coalesce(1).write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"overwrite\").save(f\"abfss://conformed@{account_name}.dfs.core.windows.net/Input/Lego/Dim_minifig/\")\n",
    "\n",
    "rename_save('Dim_minifig', minifigs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90f521b7-e440-4c29-94c4-b7297c3d7c83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2-ETL script",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
