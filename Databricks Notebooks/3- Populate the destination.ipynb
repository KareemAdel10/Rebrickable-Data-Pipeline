{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5daf3230-3692-422a-8c8b-a59d4a901be2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Setup Logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9325904c-d950-46b4-ae4a-b90e152bac16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Azure storage account details\n",
    "account_name = \"***************\"\n",
    "account_key = \"***********************\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "755af45b-077d-45db-8d44-2f30aa207b9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import lit, current_timestamp, current_date, col, xxhash64, concat, row_number, desc, when, lag\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DWH Population\").getOrCreate()\n",
    "\n",
    "\n",
    "spark.conf.set(f\"fs.azure.account.key.{account_name}.dfs.core.windows.net\", account_key)\n",
    "\n",
    "# Define base paths\n",
    "input_base_path = \"abfss://conformed@atomicatraining.dfs.core.windows.net/Input/Lego\"\n",
    "output_base_path = \"abfss://conformed@atomicatraining.dfs.core.windows.net/Output/Lego\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b5f4c18-6f6f-4c56-b104-ccd5f124b709",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# SCD type 2 implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "612b7067-9037-4dd1-b05b-eae4e7efd83d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Make Sure to only append the new and unique rows.\n",
    "    - Use `Left_anti JOIN` between both tables using a hashing column to compare between the source and target rows:\n",
    "        - If both rows have the same hash value -> ignore\n",
    "        - If both rows don't have the same hash value -> append the new record to the target table\n",
    "- Append all the source records that don't have an equivalent value as to any of the target table records.\n",
    "    - Use `row_number()` window function on all columns that have the same primary keys, order by the start_date descendingly.\n",
    "    - Inactivate all the records that don't have a _row_number_ rank = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00a60d1e-f247-4dc5-a79a-c518b3d7a409",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def upsert_to_delta(input_path, output_path, join_columns, zorder_columns):\n",
    "    \"\"\"\n",
    "    A function that handles SCD type 2 upserts\n",
    "    \"\"\"\n",
    "    source_df = spark.read.format(\"delta\").load(input_path)\n",
    "    \n",
    "    # Check if the target Delta table exists\n",
    "    if DeltaTable.isDeltaTable(spark, output_path):\n",
    "        # Load the existing Delta table\n",
    "        target_df = spark.read.format(\"delta\").load(output_path)\n",
    "        \n",
    "        #1) create a hash column:\n",
    "        df_source = source_df.withColumn(\n",
    "            \"hash64\", \n",
    "            xxhash64(concat(*[col(c) for c in source_df.columns if c not in [\"start_date\", \"end_date\", \"is_active\"]]))\n",
    "        )\n",
    "\n",
    "        df_target = target_df.withColumn(\n",
    "            \"hash64\", \n",
    "            xxhash64(concat(*[col(c) for c in target_df.columns if c not in [\"start_date\", \"end_date\", \"is_active\"]]))\n",
    "        )\n",
    "\n",
    "\n",
    "        #2) left_anti join then drop the hash column and append the result to the target table:\n",
    "        df_result = df_source.join(df_target, df_target.hash64 == df_source.hash64, 'leftanti').drop('hash64')\n",
    "        df_result = df_result.withColumn(\"end_date\", col(\"end_date\").cast(\"timestamp\"))\n",
    "        if df_result.isEmpty():\n",
    "            print(\"######> No New Records Were Detected <######\")\n",
    "        else:\n",
    "            df_result.coalesce(1).write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\").save(output_path)\n",
    "\n",
    "            # target_df.printSchema()\n",
    "            # df_result.printSchema()\n",
    "\n",
    "\n",
    "            #3)apply row_number():\n",
    "            target_df = spark.read.format(\"delta\").load(output_path)\n",
    "            target_df.createOrReplaceTempView(\"target\")\n",
    "            df_tbl = spark.sql('SELECT * FROM target')\n",
    "\n",
    "            #row_number() OVER(PARTITION BY join_columns, ORDER BY start_date desc)\n",
    "            window_def = Window.partitionBy(*[col(c) for c in join_columns]).orderBy(df_tbl.start_date.desc())\n",
    "            df_row = df_tbl.withColumn('row_number', row_number().over(window_def)) \n",
    "\n",
    "            #4) Inactivate all the rows that has a row_number > 1\n",
    "            window_row_number =  Window.partitionBy(*[col(c) for c in join_columns]).orderBy(col(\"row_number\"))\n",
    "            df_updated_end_date = df_row.withColumn(\n",
    "                'end_date', \n",
    "                when(col(\"row_number\") > 1, lag(\"start_date\",1).over(window_row_number)).otherwise(col(\"end_date\"))\n",
    "                )\n",
    "            df_final = df_updated_end_date.withColumn(\n",
    "                'is_active',\n",
    "                when(col(\"end_date\").isNotNull(), lit(False)).otherwise(col(\"is_active\"))\n",
    "                )\n",
    "            df_final = df_final.drop(\"row_number\")\n",
    "\n",
    "            df_final.coalesce(1).write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"overwrite\").save(output_path)\n",
    "            \n",
    "            # Optimize and Z-Order using Spark SQL\n",
    "            zorder_columns_str = \", \".join(zorder_columns)\n",
    "            spark.sql(f\"OPTIMIZE delta.`{output_path}` ZORDER BY ({zorder_columns_str})\")\n",
    "    else:\n",
    "        # Cast the 'start_date' in the source to 'timestamp' to match the target schema\n",
    "        source_df = source_df.withColumn(\"start_date\", col(\"start_date\").cast(\"timestamp\"))\n",
    "        # Cast the 'start_date' in the source to 'timestamp' to match the target schema\n",
    "        source_df = source_df.withColumn(\"end_date\", col(\"end_date\").cast(\"timestamp\"))\n",
    "        # If the target table does not exist, write the data to create it with schema evolution enabled\n",
    "        source_df.coalesce(1).write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"overwrite\").save(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec21a981-8a83-4942-b352-016688c4f188",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Fact_Inventory\n",
    "upsert_to_delta(\n",
    "    input_path=f\"{input_base_path}/Fact_Inventory\",\n",
    "    output_path=f\"{output_base_path}/Fact_Inventory\",\n",
    "    join_columns=[\"inventory_id\", \"set_num\", \"part_num\", \"fig_num\", \"color_id\", \"is_spare\"],\n",
    "    zorder_columns=[\"inventory_id\"]\n",
    ")\n",
    "\n",
    "# Dim_Set\n",
    "upsert_to_delta(\n",
    "    input_path=f\"{input_base_path}/Dim_sets\",\n",
    "    output_path=f\"{output_base_path}/Dim_sets\",\n",
    "    join_columns=[\"set_num\"],\n",
    "    zorder_columns=[\"set_num\"]\n",
    ")\n",
    "\n",
    "# Dim_Theme\n",
    "upsert_to_delta(\n",
    "    input_path=f\"{input_base_path}/Dim_themes\",\n",
    "    output_path=f\"{output_base_path}/Dim_themes\",\n",
    "    join_columns=[\"theme_id\"],\n",
    "    zorder_columns=[\"theme_id\"]\n",
    ")\n",
    "\n",
    "# Dim_Minifig\n",
    "upsert_to_delta(\n",
    "    input_path=f\"{input_base_path}/Dim_minifig\",\n",
    "    output_path=f\"{output_base_path}/Dim_minifig\",\n",
    "    join_columns=[\"fig_num\"],\n",
    "    zorder_columns=[\"fig_num\"]\n",
    ")\n",
    "\n",
    "# Dim_Part\n",
    "upsert_to_delta(\n",
    "    input_path=f\"{input_base_path}/Dim_parts\",\n",
    "    output_path=f\"{output_base_path}/Dim_parts\",\n",
    "    join_columns=[\"part_num\"],\n",
    "    zorder_columns=[\"part_num\"]\n",
    ")\n",
    "\n",
    "# Dim_Color\n",
    "upsert_to_delta(\n",
    "    input_path=f\"{input_base_path}/Dim_colors\",\n",
    "    output_path=f\"{output_base_path}/Dim_colors\",\n",
    "    join_columns=[\"color_id\"],\n",
    "    zorder_columns=[\"color_id\"]\n",
    ")\n",
    "\n",
    "# Dim_Part_Relationships\n",
    "upsert_to_delta(\n",
    "    input_path=f\"{input_base_path}/Dim_part_relationships\",\n",
    "    output_path=f\"{output_base_path}/Dim_part_relationships\",\n",
    "    join_columns=[\"rel_type\",\"child_part_num\", \"parent_part_num\"],\n",
    "    zorder_columns=[\"child_part_num\", \"parent_part_num\"]\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3- Populate the destination",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
